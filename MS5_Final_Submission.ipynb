{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec79bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "path = \"MS5_Final_Submission.ipynb\"\n",
    "with open(path) as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove bad widget metadata\n",
    "if 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "with open(path, 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Fixed notebook saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99244c29-a226-433e-ae8b-cfa605b80d0b",
   "metadata": {
    "id": "99244c29-a226-433e-ae8b-cfa605b80d0b"
   },
   "source": [
    "## MS5 \n",
    "Nai Hola, Enrique Hicks de Leon, Zachary Sardi-Santos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbeb7f-a84d-4032-bdf2-8fce7f6dbaef",
   "metadata": {},
   "source": [
    "## Action Items\n",
    "The notebook should read like a report to an employer with appropriate comments and explanations.\n",
    "1. Include TOC (table of contents) - in progress\n",
    "2. Describe what libraries your codes depend on. DONE\n",
    "3. If your codes require specific environment configurations, make sure to explain them. -- DONE\n",
    "4. Include an illustration/flow chart of your model. This can also be used for your presentation.\n",
    "5. Make sure a label and an explanation follows every visualization (ex. \"Figure 1.2 The above figure displays...\") DONE\n",
    "6. Cite every outside source - No need. \n",
    "\n",
    "You can move additional code to an appendix notebook if it helps make the main report notebook more concise.\n",
    "\n",
    "Notebook Report \n",
    "\n",
    "1. Group Canvas number + group member names + TOC (**need group number**)\n",
    "2. Intro: Motivation, context, and framing of the problem **done**\n",
    "3. Description of the data, how data was handled **done**\n",
    "4. EDA specific to report's content, mainly MS3 **done**\n",
    "5. Modeling approach, logical description of modeling decisions and process **kind of complete (see Final Model Section)**\n",
    "6. Results: drawing reasonable conclusions & speculations, addressing strengths, limitations, and future works **incomplete; need trading simulation**\n",
    "7. Overall communication and clarity: good visuals, minimal typos, readability\n",
    "8. Coding efficiency and style\n",
    "9. Depth and thoroughness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6368b3f-771c-4856-a86d-b050552efc52",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Introduction \n",
    "2. Data Handling\n",
    "3. EDA Report\n",
    "4. Load Preprocessed Data\n",
    "5. Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671086d8-6a13-4556-b9de-34110166d4a5",
   "metadata": {},
   "source": [
    "## Necessary Libraries\n",
    "\n",
    "Core Libraries:\n",
    "\n",
    "numpy: For numerical operations, array handling, and log return calculations\n",
    "\n",
    "pandas: For data manipulation, time series handling, and DataFrame operations\n",
    "\n",
    "matplotlib.pyplot: For visualizing distributions, cumulative returns, and portfolio performance\n",
    "\n",
    "scikit-learn: \n",
    "\n",
    "    QuantileRegressor (quantile regression baseline)\n",
    "\n",
    "    StandardScaler (feature normalization)\n",
    "\n",
    "    train_test_split (data partitioning)\n",
    "\n",
    "    Pipeline and check_array utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453b4e3-6f75-4575-b28d-977b7afc127e",
   "metadata": {},
   "source": [
    "## Specific Environment Configurations\n",
    "\n",
    "1. !pip install yfinance --> this must be completed in order to download our dataset and run the rest of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3eba7-339a-4de2-9019-c3df4ac7b5db",
   "metadata": {
    "id": "4fe3eba7-339a-4de2-9019-c3df4ac7b5db"
   },
   "source": [
    "## Introduction + Problem Statement + Task Specifics\n",
    "\n",
    "**Introduction:**\n",
    "Forecasting stock returns is a notoriously difficult task due to the noisy, nonstationary, and regime-sensitive nature of financial markets. While traditional models often rely on point forecasts (regression techniques), such predictions fail to capture the underlying uncertainty and asymmetry in return distributions — especially in volatile environments. In this project, we explore whether a Transformer-based sequence model can accurately forecast the distribution of IBM’s 5-day log returns, rather than a single value, by predicting the 10th, 50th, and 90th percentiles of the return distribution.\n",
    "\n",
    "**Problem Statement(s):**  \n",
    "Our problem statement is twofold:\n",
    "\n",
    "1. Can a Transformer, trained on rich time series features including technical indicators, macroeconomic variables, calendar effects, and regime flags (market volatility cycles), predict quantiles of log 5 day returns of IBM stock, more accurately (as measured by at least a 10% reduction in pinball loss) than a quantile regression (& GRU) baseline?\n",
    "\n",
    "2. Can these quantile forecasts translate into a profitable and risk-aware trading strategy?\n",
    "\n",
    "**Modeling Task:**\n",
    "- Quantile Regression Predict multiple quantiles (e.g., 10th, 50th, 90th percentiles) to capture the full return distribution.\n",
    "  \n",
    "**Architecture:**\n",
    "\n",
    "- A Transformer model that processes input sequences (e.g., 30-day rolling windows) and outputs the chosen target (quantiles of expected return).\n",
    "\n",
    "**Loss Functions:**  \n",
    "  - Quantile loss (pinball loss) for quantile regression.\n",
    "\n",
    "\n",
    "**Trading Simulation:**\n",
    "Once the model is trained and outputs quantiles, we have three strategies:\n",
    "1. Go long (buy) if the 10th percentile of the predicted return is > 0. even in the “worst-case” scenario, returns are expected to be positive\n",
    "   \n",
    "2. Avoid or hold cash if the 10th percentile < 0 and 50th percentile is ~ 0.\n",
    "\n",
    "3. Go Short (sell) if 90th percentile ~ 0\n",
    "\n",
    "We shall evaluate trading performance using the cumulative return and comparison against the baseline QR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363bd7dc-9e29-4b65-ac0c-6a9899ee9537",
   "metadata": {},
   "source": [
    "## Description of Data\n",
    "**Summary of Data**\n",
    "- The data is 16.6 MB in size and spans over 25 years (1998–2025) of daily observations.\n",
    "- There are 350 total features. The features range from macroeconomic (like CPI, Inflation, USD Index) to market and stock specific features (IBM price data, VIX, Sector ETF prices).\n",
    "\n",
    "**Data Handling**\n",
    "- We dropped all rows that had any NaN values and redundant OHLCV features (as explained in EDA below - post forward-fill).\n",
    "- We computed certain technical indicator features (like SMA) via their respective formulas using other features in our dataset (as explained in EDA below).\n",
    "- We standardized and took the log of all the technical indicator and market specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8dde3-a363-4444-95be-bbe914305d2a",
   "metadata": {
    "id": "7ab8dde3-a363-4444-95be-bbe914305d2a"
   },
   "source": [
    "## Shifts in Task Specific (Modeling) Approach Via EDA\n",
    "\n",
    "We chose to pursue the quantile regression method after observing our target variable, log 5 day returns, was approximately normal centered around zero with a thicker left tail. This justifies quantile regression as a predictive task, because the lower‑end quantiles, like 10th percentile quantile, have the most risk information. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/5_day_return_histogram.png\" alt=\"Volume Boxplot\" style=\"width:700px;\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a612cc-4691-44c2-8efa-945cee0ad2d2",
   "metadata": {
    "id": "74a612cc-4691-44c2-8efa-945cee0ad2d2"
   },
   "source": [
    "## Comprehensive EDA Review\n",
    "\n",
    "## 0. Key Findings\n",
    "1. **Multicollinearity --> Feature Reduction**\n",
    "EDA revealed extreme multicollinearity among OHLCV components (Open, High, Low, Close, Volume) for each ticker, in many cases with correlations exceeding 0.95 (e.g., SP500_Open vs SP500_High = 0.999).\n",
    "- We retained only the Close price per ticker as the core price signal.\n",
    "- All technical indicators were derived from these Close prices.\n",
    "- This reduced the number of raw price features while maintaining their trend and volatility information through derived signals.\n",
    "\n",
    "2. **Heavy-tailed, Skewed Feature Distributions --> Standard Scaling**\n",
    "The histograms and boxplots showed:\n",
    "- Volume, volatility, and momentum indicators (e.g., ATR, RSI, OBV) were right-skewed distributions.\n",
    "- Several indicators had extreme outliers, especially during crisis periods (2008 crisis or 2020 COVID pandemic).\n",
    "- To prevent these features from dominating training, we will apply log transformations to features like volume and ATR to stabilize variance. We will also use StandardScaler to ensure features had similar scales and were less sensitive to outliers.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/volume_boxplot.png\" alt=\"Volume Boxplot\" style=\"width:410px;\">                                   \n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/volume_histogram.png\" alt=\"Volume Histogram\" style=\"width:500px;\">\n",
    "</p>\n",
    "\n",
    "3. **Time-Lagged Lead Effect --> 30 day Window**\n",
    "- Momentum indicators like RSI, MACD, ROC had the strongest correlation to 5 day log returns at 1–2 day lags. Essentially, Momentum indictators from two days prior had the strongest correlation to present day 5 day log return values\n",
    "- Volatility indicators (e.g., rolling std, ATR) remained predictive over 5–7 days. Essentially, volatility indicators carry signal a week out.\n",
    "- Essentially, this pointed to the fact using a 30 day window to predict future returns is a smart idea, because of this lagged correlation. We can capture short term momentum effects (signals that work at 1-2 day lags) and retain medium-horizon volatility context (weekly signal) and give the model temporal depth to learn signifiance and correlation of features over time.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/lagged_correlation.png\" alt=\"Volume Boxplot\" style=\"width:500px;\">                                   \n",
    "</p>\n",
    "\n",
    "4. **Market Regimes Exists (via PCA and k-means) --> Need Contextual Features**\n",
    "- PCA + k-means clustering revealed that the dataset naturally splits into three regimes: Calm, Trending, High Volatility\n",
    "- Now each data point is labeled with its regime cluster, which is embedded in the Transformer model.\n",
    "- This will help the model adapt to different market behaviours.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/market_regimes.png\" alt=\"Volume Boxplot\" style=\"width:500px;\">                                   \n",
    "</p>\n",
    "\n",
    "5. **Calendar Effects --> Weekday Dummies**\n",
    "We observed slightly higher returns on Fridays and Tuesdays, consistent with known market anomalies.\n",
    "- We added one-hot encoded weekday dummies (is_monday, is_tuesday, etc.) to help the model account for these weekday effects.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/enriquedlh97/109b-ms4-images/main/weekday_return.png\" alt=\"Volume Boxplot\" style=\"width:500px;\">                                   \n",
    "</p>\n",
    "\n",
    "## 1. Handling Missing Values\n",
    "\n",
    "- **Issue:**  \n",
    "  Macroeconomic indicators like CPI and Unemployment are reported monthly. When these series are resampled to daily frequency, the days before the first report in our range result in NaNs.\n",
    "\n",
    "- **Solution:**  \n",
    "  We applied **forward-fill** to propagate the most recent available value forward. In addition, **back-fill** was used after the forward-fill step to fill any remaining gaps. This ensures that the final dataset has continuous daily values without missing data points, which is essential for stable model training.\n",
    "\n",
    "## 2. Feature Engineering\n",
    "\n",
    "- **Feature Reduction via Correlation Analysis**\n",
    "- Check above (reduced OHLCV to just Close)\n",
    "\n",
    "**Technical Indicator Computation (all computed from IBM Close Price)**\n",
    "- **Momentum Indicators**:\n",
    "- RSI (Relative Strength Index, 14-day): Measures recent gain/loss strength\n",
    "- MACD: Difference between 12-day EMA and 26-day EMA\n",
    "- ROC (Rate of Change, 10-day): Percentage change in price\n",
    "\n",
    "- **Volatility Indicators**:\n",
    "- Rolling standard deviation (20-day): Measures price variability\n",
    "- ATR (Average True Range, 14-day): Captures range-based volatility\n",
    "- Bollinger Bands: Upper and lower bounds around a 20-day SMA ± 2σ\n",
    "\n",
    "- **Trend Indicators**:\n",
    "- SMA & EMA (20-day and 50-day): Smooth trend signals\n",
    "\n",
    "**Volume-Based Indicators**:\n",
    "- OBV (On-Balance Volume): Cumulative measure of volume flow\n",
    "- MFI (Money Flow Index, 14-day): Volume-weighted momentum\n",
    "\n",
    "**IBM Return Features (1-day, 5-day, 10-day, 20-day rolling mean)**\n",
    "- Computed from IBM close price\n",
    "\n",
    "**Rolling Statistics for Time-Dependent Features**\n",
    "- For volatility and momentum: Applied rolling windows (e.g., 14, 20, 50 days)\n",
    "\n",
    "**One hot encoded weekday dummies**\n",
    "- Capture calendar anomalies, so will input one hot encoded weekdays into Transformer\n",
    "\n",
    "**Labelled Each Observation in DF with market regime based on PCA & Clustering**\n",
    "- Market behaviour impacts return, so added market regime label to each observation to capture this affect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d6232-8d02-4162-b000-737ddef3dd77",
   "metadata": {
    "id": "8f5d6232-8d02-4162-b000-737ddef3dd77"
   },
   "source": [
    "## Data Processing For Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d348457-45ff-462c-8243-873a240d259c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install yfinance pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739caec-2991-4fde-a65b-413ebe23fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf \n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1115d-b82b-4954-a311-831f4e431696",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_csv(\"processed_data.csv\", index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe5b23-5932-4a6b-9ef2-6f4d458cec0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fef9d3-3e58-4d5a-8310-bf39b1e0e185",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "We will build our own lightweight Transformer model for this project. This is because any pre-trained transformer models from HuggingFace are trained on language and text corpora, which are not compatible with our continuous time series data.\n",
    "\n",
    "Model Architecture\n",
    "1. Inputs: batch of 30 day sequences\n",
    "2. Embedding projection (Dense): compresses raw numeric features into an FFN-dimension (ff_dim), analogous to word/token embeddings. Allows model to learn which raw signals matter the most before attention. Compression is also efficient. \n",
    "3. Transformer Stack: Multi head attention - stacks multiple transformer encoders on top of one another, w/ num_layer stacks\n",
    "4. GlobalAvgPooling1D: Collapses sequences into one vector\n",
    "5. Output (Dense): One output value per quantile (so dense of 3)\n",
    "\n",
    "The reasoning for this model architecture follows that of our GPT and ChatGPT lectures (lectures 17 and 18) and our HW5 parts 2-3. The only notable switch is using a GlobalAvgPooling layer instead of a CLS token concatenation. This is because our task is regression, not classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bce44d-8199-4c16-a55d-5578e0cd2146",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block Function + Custom Output Layer MHA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a835a9-d2a2-4043-a2f8-ed2ca982d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.set_random_seed(109)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "#transformer encoder block function \n",
    "def transformer_encoder_block(x,num_heads,\n",
    "                              ff_dim,\n",
    "                              dropout_rate, ffn_dropout_rate=0.1,\n",
    "                              attn_dropout_rate=0.1 ):\n",
    "    #first create attention output embeddings which use tf.layers MHA attribute\n",
    "    #we do not need to create embeddings because the data is already numerical vectors\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads = num_heads,\n",
    "        key_dim = x.shape[-1])(x,x) \n",
    "    #key_dim is the dimension of key and query matrices and should be x.shape[-1] because there will be a \"u\"\n",
    "    #for every token which I assume is each numeric value in the data vector \n",
    "    #(x,x) indicates self-attention \n",
    "    # (query, value) = (x, x) for self-attentio\n",
    "\n",
    "    #skip connection, dropout, and layer norm\n",
    "    #.Add() is the skip connection attribute in tf \n",
    "    #do layernorm and dropout next\n",
    "    x = tf.keras.layers.Add()([x, attention_output])\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    #first dense layer expands to ff_dim \n",
    "    ffn_output = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    #second dense layer projects back to original embed dimension \n",
    "    #expand --> nonlinearity --> project which is typical I guess\n",
    "    \n",
    "    ffn_output = tf.keras.layers.Dense(x.shape[-1])(ffn_output)\n",
    "    \n",
    "    #additional residual and layer norm and dropout\n",
    "    x = tf.keras.layers.Add()([x, attention_output])\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f10357-f46b-4ec1-8366-4a41bef89d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build actual transformer quantile model function\n",
    "def build_transformer_quantile_model(\n",
    "    input_shape, #input_shape will be a tuple (30 days, # of features) assuming we pass in all the features\n",
    "    num_heads, #number of attention heads in the self attention layer\n",
    "    ff_dim, #how many neurons in dense layer\n",
    "    num_layers, #how many transformer encoder blocks to stack -- how many should we do? \n",
    "    dropout_rate, #dropout rate for a layer\n",
    "    kernel_regularizer_strength, #L2 regularization on Dense layers kernels\n",
    "    bias_regularizer_strength, #L2 regularization o nDense layer biases\n",
    "    quantiles = [0.1, 0.5, 0.9]): #quantiles to predict/output\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape) #initialize the inputs\n",
    "\n",
    "    #this Dense layer actually projects the raw features into higher dimensional embedding space (ff_dim)\n",
    "    #it is like the token embedding\n",
    "    x = tf.keras.layers.Dense(\n",
    "        ff_dim,\n",
    "        kernel_regularizer = tf.keras.regularizers.l2(kernel_regularizer_strength),\n",
    "        bias_regularizer = tf.keras.regularizers.l2(bias_regularizer_strength)\n",
    "    )(inputs)\n",
    "\n",
    "    #stack the transformer encoder blocks on top of one another, num_layer stacks\n",
    "    #for _ in range(num_layers) and set x = transformer encoder block function\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder_block(x, num_heads = num_heads, ff_dim = ff_dim,\n",
    "                                      dropout_rate=dropout_rate)\n",
    "    #we use global average pooling to average over 30 days, flatten into one giant vector\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    #will output a value for each quantile (this is why the number of neurons is len(quantile)\n",
    "    outputs = tf.keras.layers.Dense(len(quantiles))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0bc89-da7a-4b03-bf41-664d5de33998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building transformer model for presentation\n",
    "def build_transformer_quantile_model(\n",
    "    input_shape, \n",
    "    num_heads, \n",
    "    ff_dim, \n",
    "    num_layers,  \n",
    "    dropout_rate, \n",
    "    kernel_regularizer_strength, \n",
    "    bias_regularizer_strength, \n",
    "    quantiles = [0.1, 0.5, 0.9]): \n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        ff_dim,\n",
    "        kernel_regularizer = tf.keras.regularizers.l2(kernel_regularizer_strength),\n",
    "        bias_regularizer = tf.keras.regularizers.l2(bias_regularizer_strength)\n",
    "    )(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder_block(x, num_heads = num_heads, ff_dim = ff_dim,\n",
    "                                      dropout_rate=dropout_rate)\n",
    "        \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "   \n",
    "    outputs = tf.keras.layers.Dense(len(quantiles))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40469ff0-4dd9-461a-a3a6-132c1128d9aa",
   "metadata": {},
   "source": [
    "## Creating 30 Day Input Sequences of The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3ea5d-c295-444f-b67d-53e55fae2a38",
   "metadata": {
    "id": "edd3ea5d-c295-444f-b67d-53e55fae2a38"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def create_timeseries_windows_with_metadata(df, window_size=30, target_col='IBM_Return_5d', price_col='IBM_Close_raw'):\n",
    "    \"\"\"\n",
    "    Create time series windows for model input and collect metadata for realistic trading simulation.\n",
    "\n",
    "    Args:\n",
    "        df: pd.DataFrame, must include target_col and price_col, and have a datetime index.\n",
    "        window_size: int, number of past days in each input window.\n",
    "        target_col: str, name of the target column.\n",
    "        price_col: str, name of the close price column used for real trading.\n",
    "\n",
    "    Returns:\n",
    "        X: np.array of input sequences\n",
    "        y: np.array of targets\n",
    "        metadata: pd.DataFrame with entry and exit info\n",
    "    \"\"\"\n",
    "    feature_cols = df.columns.difference([target_col, price_col])\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    metadata = []\n",
    "\n",
    "    for i in tqdm(range(len(df) - window_size - 5)):\n",
    "        window_features = df.iloc[i:i+window_size][feature_cols].values\n",
    "        target_value = df.iloc[i + window_size + 4][target_col]\n",
    "\n",
    "        # Store input and target\n",
    "        X.append(window_features)\n",
    "        y.append(target_value)\n",
    "\n",
    "        # Now also store trading metadata\n",
    "        entry_idx = i + window_size\n",
    "        exit_idx = i + window_size + 5\n",
    "\n",
    "        entry_date = df.index[entry_idx]\n",
    "        exit_date = df.index[exit_idx]\n",
    "\n",
    "        entry_close = df.iloc[entry_idx][price_col]\n",
    "        exit_close = df.iloc[exit_idx][price_col]\n",
    "\n",
    "        metadata.append({\n",
    "            \"entry_idx\": entry_idx,\n",
    "            \"exit_idx\": exit_idx,\n",
    "            \"entry_date\": entry_date,\n",
    "            \"exit_date\": exit_date,\n",
    "            \"entry_close\": entry_close,\n",
    "            \"exit_close\": exit_close\n",
    "        })\n",
    "\n",
    "    return np.array(X), np.array(y), pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a456a7-c7b8-4715-87e1-18a197a70123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, metadata = create_timeseries_windows_with_metadata(\n",
    "    processed_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda9995-4908-4995-9e3e-64ae59ba0b2c",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654e162-63d8-4cac-8577-b64f3dcd778a",
   "metadata": {},
   "source": [
    "The tf.keras.compile attribute does not take as input a pinball loss, so we must define our own unique pinball loss function in order to pass it as a valid attribute into compile.\n",
    "\n",
    "Quantile loss is asymmetric; \n",
    "L = max(q(y-y_hat), 1-q(y-y_hat)), so there are two cases:\n",
    "1. actual outcome is higher than prediction. Loss = q(y-y_hat). If q is small, say 0.1, we only pay a little in terms of loss if we underestimate for this quantile. Which makes sense, because it is our 10%, worst case scenario, so we would hope that the return value for this 10% quantile is lower than the actual return. \n",
    "2. actual outcome is lower than prediction. If q1 is small, say 0.1, we pay a lot in terms of loss because the (1-q) term is 0.9.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1b2d5-390b-4d31-b16c-c51830cd88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "def make_quantile_loss_fn(quantiles):\n",
    "    #creates a tf tensor based on the quantiles\n",
    "    quantiles = tf.constant(quantiles, dtype=tf.float32)\n",
    "    \n",
    "    def quantile_loss(y_true, y_pred):\n",
    "        #y_true expanded is just the actual 5day log return expanded a dimension so we can compute the loss\n",
    "        y_true_expanded = tf.expand_dims(y_true, axis=-1)\n",
    "\n",
    "        #compoute error between actual return and the predicted return based on the quantile \n",
    "        err = y_true_expanded - y_pred  \n",
    "\n",
    "        # Broadcast quantiles: shape (Q,) → (batch, Q)\n",
    "        q = quantiles[None, :]\n",
    "\n",
    "        # Pinball: max(q * err, (q-1) * err)\n",
    "        loss = tf.maximum(q * err, (q - 1) * err)\n",
    "        #essentially what the pinball loss computes is how far our quantile predictions are from the actual return\n",
    "        #so like if the 10% quantile prediction is much greater than the actual return, we overestimated the return \n",
    "        #(specifically, we underestimated the risk)\n",
    "        #\n",
    "        \n",
    "\n",
    "        # Return average over batch *and* over all quantiles\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    return quantile_loss\n",
    "\n",
    "\n",
    "\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "pinball_loss_fn = make_quantile_loss_fn(quantiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d4f20-eaa9-497e-aa5a-93143e086cae",
   "metadata": {},
   "source": [
    "## Formulating the Data into Tensor Architecture For Improved Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aea4b9-6898-455b-a0ad-cab9b3bc9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "train_end = int(n*0.7)\n",
    "val_end   = int(n*0.85)\n",
    "\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val,   y_val   = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test,  y_test  = X[val_end:],     y[val_end:]\n",
    "metadata_test = metadata[val_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833d5c-becf-4d3d-a1d0-ba19d1501987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def make_dataset(X, y, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X), reshuffle_each_iteration=True)\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val,   y_val,   shuffle=False)\n",
    "test_ds  = make_dataset(X_test,  y_test,  shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a996ea6-e561-47f0-b4fe-7dd79d23bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(train_ds.take(1).element_spec[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959013d-8f48-4845-a8a3-d55f07ee3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(train_ds.take(1).element_spec[0].shape)\n",
    "model = build_transformer_quantile_model(\n",
    "    input_shape=(30, 347),  # 30-day window, 350 features\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout_rate=0.01,\n",
    "    kernel_regularizer_strength=0.00001,\n",
    "    bias_regularizer_strength=0.00001,\n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec473b00-4339-4b23-a138-53160de50d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# steps_per_epoch = len(processed_data)\n",
    "# lr_schedule = CosineDecay(\n",
    "#     initial_learning_rate=0.005,\n",
    "#     decay_steps=20 * steps_per_epoch,  # total training steps\n",
    "#     alpha=1e-5  # final learning rate as a fraction of initial\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.005)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=pinball_loss_fn,\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "callbacks_list = [early_stopping, reduce_lr]\n",
    "\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = 100, verbose = 1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932198a7-9031-4e4c-bd09-add91e10f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_ds, verbose = 1)\n",
    "print(f\"Test pinball loss: {test_results[0]:.6f}\")\n",
    "print(f\"Test MAE (median): {test_results[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64d994-5f5d-42d7-a3a3-2e3da66e9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2819093-9137-4759-acf8-6f49712b1dab",
   "metadata": {},
   "source": [
    "## Simulated Trading Strategy Step 1: Compile DataFrame of Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7bdc9-c874-4121-94dc-cc10b43ba6c9",
   "metadata": {},
   "source": [
    "The first step is to compile a dataframe of all the predicted quantiles given 30 day sequences of the test dataset (test_ds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f306aa3-67ec-4b66-bbf2-113d222a98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list, y_pred_list = [], []\n",
    "for X_batch, y_batch in test_ds:  #tuple loop, gives x_batch and y_batch pair\n",
    "    batch_preds = model.predict(X_batch, verbose = 0)\n",
    "    y_pred_list.append(batch_preds) #this will be a triple list addition because\n",
    "    #model.predict will return 3 values b/c Dense output is 3 neurons\n",
    "    y_true_list.append(y_batch.numpy())\n",
    "\n",
    "y_test_all = np.concatenate(y_true_list, axis = 0)\n",
    "#q_preds_all[:,0] is 10% forecast, [:,1] the 50% (median), [:,2] the 90% forecast.\n",
    "q_preds_all = np.vstack(y_pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8297c9-044d-4885-9a85-49bee337853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of predicted quantiles and actual return\n",
    "transformer_preds_df = pd.DataFrame({\n",
    "    'q10':           q_preds_all[:, 0],\n",
    "    'q50':           q_preds_all[:, 1],\n",
    "    'q90':           q_preds_all[:, 2],\n",
    "    'Actual_5d_Return': y_test_all\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c284de-1433-40be-9363-24b5c3d33f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.hist(transformer_preds_df['q10'], bins=50, alpha=1, label='Predicted 10th Percentile')\n",
    "plt.hist(transformer_preds_df['q50'], bins=50, alpha=1, label='Predicted 50th Percentile')\n",
    "plt.hist(transformer_preds_df['q90'], bins=50, alpha=1, label='Predicted 90th Percentile')\n",
    "plt.hist(transformer_preds_df['Actual_5d_Return'], bins=50, alpha=0.2, label='Actual 5-Day Return')\n",
    "\n",
    "plt.title('Distribution of Predicted Quantiles vs Actual Returns')\n",
    "plt.xlabel('Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666420b-84d9-4d44-80a5-4f55f5c60cb2",
   "metadata": {},
   "source": [
    "## Simulated Trading Strategy V2: Full Backtest Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41460c-8e32-4cfb-9bcf-82418d5fe7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ibm_price_with_trade_markers(metadata_frame, trade_marker_list):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(pd.to_datetime(metadata_frame[\"entry_date\"]),\n",
    "             metadata_frame[\"entry_close\"],\n",
    "             color=\"black\", label=\"IBM Close Price\")\n",
    "\n",
    "    for marker in trade_marker_list:\n",
    "        plt.scatter(marker[\"date\"], marker[\"price\"],\n",
    "                    marker=\"^\" if marker[\"side\"] == \"buy\" else \"v\",\n",
    "                    color=\"green\" if marker[\"side\"] == \"buy\" else \"red\",\n",
    "                    s=85)\n",
    "\n",
    "    plt.title(\"IBM Close Price with Trade Signals\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_portfolio_and_benchmark(trading_series, buy_hold_series):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(trading_series.index, trading_series.values,\n",
    "             label=\"Trading Portfolio\", color=\"blue\")\n",
    "    plt.plot(buy_hold_series.index, buy_hold_series.values,\n",
    "             label=\"Buy & Hold Benchmark\", color=\"orange\", linestyle=\"--\")\n",
    "    plt.title(\"Trading Portfolio vs Buy-and-Hold\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Portfolio Value ($)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_basic_metrics(portfolio_series, initial_cash_amount):\n",
    "    daily_returns = portfolio_series.pct_change().dropna()\n",
    "    total_return = portfolio_series.iloc[-1] / initial_cash_amount - 1.0\n",
    "    days_covered = (portfolio_series.index[-1] -\n",
    "                    portfolio_series.index[0]).days\n",
    "    annualised_return = (1.0 + total_return) ** (365.0 / days_covered) - 1.0\n",
    "    maximum_drawdown = (portfolio_series / portfolio_series.cummax() -\n",
    "                        1.0).min()\n",
    "    annualised_volatility = daily_returns.std() * np.sqrt(252)\n",
    "    final_portfolio_value = portfolio_series.iloc[-1]\n",
    "    return (total_return, annualised_return,\n",
    "            maximum_drawdown, annualised_volatility, final_portfolio_value)\n",
    "\n",
    "def run_quantile_backtest(\n",
    "    prediction_frame,\n",
    "    metadata_frame,\n",
    "    *,\n",
    "    initial_cash_amount=100_000,\n",
    "    cash_fraction_at_risk=0.5,\n",
    "    commission_rate_per_side=0.001,\n",
    "    bias_buffer_normal_return=0.002, # >= 0.2 % median to choose a direction\n",
    "    skew_buffer_normal_return=0.002, # >= 0.2 % extra tail in same direction\n",
    "    stop_loss_enabled=True,\n",
    "    holding_period_rows=5,\n",
    "    plot_results=True\n",
    "):\n",
    "\n",
    "    prediction_frame = prediction_frame.reset_index(drop=True)\n",
    "    metadata_frame = metadata_frame.reset_index(drop=True)\n",
    "    bias_buffer_log_return = np.log1p(bias_buffer_normal_return)\n",
    "    skew_buffer_log_return = np.log1p(skew_buffer_normal_return)\n",
    "\n",
    "    # buy and hold\n",
    "    initial_close_price = metadata_frame[\"entry_close\"].iloc[0]\n",
    "    benchmark_share_quantity = initial_cash_amount / initial_close_price\n",
    "    buy_hold_series = metadata_frame[\"entry_close\"] * benchmark_share_quantity\n",
    "    buy_hold_series.index = pd.to_datetime(metadata_frame[\"entry_date\"])\n",
    "\n",
    "    cash_balance = float(initial_cash_amount)\n",
    "    equity_curve_values, equity_curve_dates = [], []\n",
    "    total_commission_paid = 0.0\n",
    "    open_position = None\n",
    "    trade_marker_list = []\n",
    "\n",
    "    total_rows = len(prediction_frame)\n",
    "\n",
    "    for row_index in range(total_rows):\n",
    "        quantile_10, quantile_50, quantile_90 = \\\n",
    "            prediction_frame.loc[row_index, [\"q10\", \"q50\", \"q90\"]]\n",
    "        current_date = metadata_frame.loc[row_index, \"entry_date\"]\n",
    "        current_close_price = metadata_frame.loc[row_index, \"entry_close\"]\n",
    "\n",
    "        # close position if holding period is complete\n",
    "        if open_position and row_index == open_position[\"exit_row_index\"]:\n",
    "            direction_of_trade = open_position[\"direction\"]\n",
    "            share_quantity = open_position[\"share_quantity\"]\n",
    "            cash_used_for_trade = open_position[\"cash_used_for_trade\"]\n",
    "\n",
    "            market_value_now = share_quantity * current_close_price\n",
    "            profit_or_loss = (market_value_now - cash_used_for_trade) * direction_of_trade\n",
    "            exit_commission = market_value_now * commission_rate_per_side\n",
    "            total_commission_paid += exit_commission\n",
    "\n",
    "            if direction_of_trade == 1:\n",
    "                cash_balance += cash_used_for_trade\n",
    "            cash_balance += profit_or_loss - exit_commission\n",
    "\n",
    "            trade_marker_list.append({\n",
    "                \"date\": current_date,\n",
    "                \"price\": current_close_price,\n",
    "                \"side\": \"sell\" if direction_of_trade == 1 else \"buy\"\n",
    "            })\n",
    "            open_position = None\n",
    "\n",
    "        # stop-loss\n",
    "        if open_position and stop_loss_enabled:\n",
    "            direction_of_trade = open_position[\"direction\"]\n",
    "            entry_price = open_position[\"entry_price\"]\n",
    "            cumulative_return = direction_of_trade * (current_close_price / entry_price - 1.0)\n",
    "\n",
    "            if (direction_of_trade == 1 and cumulative_return < quantile_10) or \\\n",
    "               (direction_of_trade == -1 and cumulative_return > quantile_90):\n",
    "                # close early\n",
    "                market_value_now = open_position[\"share_quantity\"] * current_close_price\n",
    "                profit_or_loss = (market_value_now - open_position[\"cash_used_for_trade\"]) * direction_of_trade\n",
    "                exit_commission = market_value_now * commission_rate_per_side\n",
    "                total_commission_paid += exit_commission\n",
    "\n",
    "                if direction_of_trade == 1:\n",
    "                    cash_balance += open_position[\"cash_used_for_trade\"]\n",
    "                cash_balance += profit_or_loss - exit_commission\n",
    "\n",
    "                trade_marker_list.append({\n",
    "                    \"date\": current_date,\n",
    "                    \"price\": current_close_price,\n",
    "                    \"side\": \"sell\" if direction_of_trade == 1 else \"buy\"\n",
    "                })\n",
    "                open_position = None\n",
    "\n",
    "        # open new position\n",
    "        if open_position is None and row_index + holding_period_rows < total_rows:\n",
    "            direction_signal = 0\n",
    "            if (quantile_50 > bias_buffer_log_return and\n",
    "                    (quantile_90 - quantile_50) >= skew_buffer_log_return):\n",
    "                direction_signal = 1 # long\n",
    "            elif (quantile_50 < -bias_buffer_log_return and\n",
    "                    (quantile_50 - quantile_10) >= skew_buffer_log_return):\n",
    "                direction_signal = -1# short\n",
    "\n",
    "            if direction_signal != 0:\n",
    "                cash_to_invest = cash_balance * cash_fraction_at_risk\n",
    "                share_quantity = cash_to_invest / current_close_price\n",
    "                entry_commission = cash_to_invest * commission_rate_per_side\n",
    "                total_commission_paid += entry_commission\n",
    "\n",
    "                if direction_signal == 1: # pay for shares on long\n",
    "                    cash_balance -= cash_to_invest\n",
    "                cash_balance -= entry_commission\n",
    "\n",
    "                open_position = {\n",
    "                    \"direction\": direction_signal,\n",
    "                    \"share_quantity\": share_quantity,\n",
    "                    \"cash_used_for_trade\": cash_to_invest,\n",
    "                    \"entry_price\": current_close_price,\n",
    "                    \"exit_row_index\": row_index + holding_period_rows\n",
    "                }\n",
    "\n",
    "                trade_marker_list.append({\n",
    "                    \"date\": current_date,\n",
    "                    \"price\": current_close_price,\n",
    "                    \"side\": \"buy\" if direction_signal == 1 else \"sell\"\n",
    "                })\n",
    "\n",
    "        portfolio_value_today = cash_balance\n",
    "        if open_position:\n",
    "            direction_of_trade = open_position[\"direction\"]\n",
    "            share_quantity = open_position[\"share_quantity\"]\n",
    "            position_value = share_quantity * current_close_price * direction_of_trade\n",
    "            if direction_of_trade == -1: # add margin deposit for short\n",
    "                portfolio_value_today += open_position[\"cash_used_for_trade\"]\n",
    "            portfolio_value_today += position_value\n",
    "\n",
    "        equity_curve_dates.append(current_date)\n",
    "        equity_curve_values.append(portfolio_value_today)\n",
    "\n",
    "    trading_portfolio_series = pd.Series(equity_curve_values,\n",
    "                                         index=pd.to_datetime(equity_curve_dates))\n",
    "\n",
    "    if plot_results:\n",
    "        plot_ibm_price_with_trade_markers(metadata_frame, trade_marker_list)\n",
    "        plot_portfolio_and_benchmark(trading_portfolio_series, buy_hold_series)\n",
    "\n",
    "    trading_metrics = compute_basic_metrics(trading_portfolio_series, initial_cash_amount)\n",
    "    benchmark_metrics = compute_basic_metrics(buy_hold_series, initial_cash_amount)\n",
    "\n",
    "    def print_metrics_block(label, metrics_tuple):\n",
    "        total_ret, annual_ret, max_dd, annual_vol, final_val = metrics_tuple\n",
    "        print(f\"\\n\\t {label} \")\n",
    "        print(f\"Final value:          ${final_val:,.2f}\")\n",
    "        print(f\"Total return:         {total_ret:.2%}\")\n",
    "        print(f\"Annualised return:    {annual_ret:.2%}\")\n",
    "        print(f\"Maximum draw-down:    {max_dd:.2%}\")\n",
    "        print(f\"Annual volatility:    {annual_vol:.2%}\")\n",
    "\n",
    "    print(f\"\\nTotal commissions paid: ${total_commission_paid:,.2f}\")\n",
    "    print_metrics_block(\"Trading Portfolio\", trading_metrics)\n",
    "    print_metrics_block(\"Buy-and-Hold Benchmark\", benchmark_metrics)\n",
    "\n",
    "    return {\n",
    "        \"trading_portfolio\": trading_portfolio_series,\n",
    "        \"buy_hold_portfolio\": buy_hold_series,\n",
    "        \"total_commissions_paid\": total_commission_paid\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7cd93-5a23-40fb-9c18-2294b1631a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_quantile_backtest(\n",
    "    prediction_frame=transformer_preds_df,\n",
    "    metadata_frame=metadata_test,\n",
    "    initial_cash_amount=100_000,\n",
    "    cash_fraction_at_risk=1,\n",
    "    commission_rate_per_side=0.0001,\n",
    "    bias_buffer_normal_return=0.00057,\n",
    "    skew_buffer_normal_return=0.03,\n",
    "    stop_loss_enabled=True,\n",
    "    holding_period_rows=5,\n",
    "    plot_results=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e21a0f-c552-4858-b42f-3b7b1ea7236a",
   "metadata": {
    "id": "26e21a0f-c552-4858-b42f-3b7b1ea7236a"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020d52a-9a18-4437-8738-4151ed6ca3dd",
   "metadata": {
    "id": "4020d52a-9a18-4437-8738-4151ed6ca3dd"
   },
   "source": [
    "The Baseline Model has two options\n",
    "1. A Two-Fold model, similar to our transformer -- models the returns distribtution across x day sequence length and simulates a trading strategy based on that distribution.\n",
    "2. Two singular models, one that models the returns distribution across x day sequence length, and another that simulates a trading strategy based on a different technique, such as \"Moving Crossover SMA\".\n",
    "\n",
    "We will pursue the first option as we believe having a baseline model that essentially runs the same functional process as our Transformer will be best for evaluating and measuring performance -- as two separate baseline models will not simulate trades based on the return distribution, which is a task we aim to accomplish with our Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbab8e4-a7b1-40b2-b842-b4233c55717d",
   "metadata": {
    "id": "7fbab8e4-a7b1-40b2-b842-b4233c55717d"
   },
   "source": [
    "**Quantile Regression Baseline Model (30 Day Input Sequence, Model Distribution of 5 Day Returns)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9172b53-05f9-406c-8f2b-75e99ef96b2c",
   "metadata": {
    "id": "c9172b53-05f9-406c-8f2b-75e99ef96b2c"
   },
   "source": [
    "Since Quantile Regression is a static model that cannot model lengthy sequences, we summarize each 30-day feature window into a flat feature vector via rolling mean operations. This allows us to retain the most important statistical properties of the past 30 days in a form that Quantile Regression can interpret.\n",
    "\n",
    "X_mean is an array of all the mean values of all the 30 day sequences created. Shape (6145, 349)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12425374-ae6e-4d72-92cd-7d11f3edb1f7",
   "metadata": {
    "id": "12425374-ae6e-4d72-92cd-7d11f3edb1f7"
   },
   "outputs": [],
   "source": [
    "X_mean = X.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df722b48-7f97-4142-ba04-17019e4b68f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df722b48-7f97-4142-ba04-17019e4b68f6",
    "outputId": "90364471-7f01-4988-a41b-bc4794a5933f"
   },
   "outputs": [],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1932cd-e438-42ed-b2b8-2fa948f3a6fa",
   "metadata": {
    "id": "ac1932cd-e438-42ed-b2b8-2fa948f3a6fa"
   },
   "source": [
    "## Quantile Regression Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af38bb1-a313-430d-8479-e3aaac004485",
   "metadata": {
    "id": "1af38bb1-a313-430d-8479-e3aaac004485"
   },
   "source": [
    "Have to drop NaN rows as there was a problem with technical indicators having NaN values with the 30-day sequences.\n",
    "\n",
    "GPT explanation - \"Some original technical indicators (like RSI, MACD, ATR) naturally produce NaN values in their initial periods (e.g., first 14 days, 20 days).\n",
    "\n",
    "When you build 30-day windows, early windows that touch NaNs infect your features.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145b77b-0374-4019-b796-10f4c134d775",
   "metadata": {
    "id": "a145b77b-0374-4019-b796-10f4c134d775"
   },
   "outputs": [],
   "source": [
    "# combine into one DataFrame\n",
    "temp_df = pd.DataFrame(X_mean)\n",
    "temp_df['target'] = y\n",
    "\n",
    "# drop rows with any NaN\n",
    "temp_df = temp_df.dropna()\n",
    "\n",
    "# Separate back into X and y\n",
    "X_mean_clean = temp_df.drop(columns='target').values\n",
    "y_clean = temp_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda82ef-f6ad-461d-9019-7a4f2d5b540c",
   "metadata": {
    "id": "8fda82ef-f6ad-461d-9019-7a4f2d5b540c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(X_mean_clean, y_clean, metadata, test_size=0.15, shuffle=False)\n",
    "\n",
    "# (shuffle=False because it's time series)\n",
    "\n",
    "#fit model on training data\n",
    "model_q10 = QuantileRegressor(quantile=0.1, alpha=0).fit(X_train, y_train)\n",
    "model_q50 = QuantileRegressor(quantile=0.5, alpha=0).fit(X_train, y_train)\n",
    "model_q90 = QuantileRegressor(quantile=0.9, alpha=0).fit(X_train, y_train)\n",
    "\n",
    "#predict on test data\n",
    "q10_preds = model_q10.predict(X_test)\n",
    "q50_preds = model_q50.predict(X_test)\n",
    "q90_preds = model_q90.predict(X_test)\n",
    "\n",
    "#build prediction dataframe for plotting\n",
    "preds_df = pd.DataFrame({\n",
    "    'q10': q10_preds,\n",
    "    'q50': q50_preds,\n",
    "    'q90': q90_preds,\n",
    "    'Actual_5d_Return': y_test\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751b381-077b-4d66-836d-7761194161fe",
   "metadata": {},
   "source": [
    "## Quantile Baseline Predicting Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df621ca-5cf1-465a-8cf4-d6daddafee6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "0df621ca-5cf1-465a-8cf4-d6daddafee6b",
    "outputId": "65e3ac1c-5f61-41a7-9491-4393b11f7d65"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "#plot predicted quantiles\n",
    "plt.hist(preds_df['q10'], bins=50, alpha=0.5, label='Predicted 10th Percentile')\n",
    "plt.hist(preds_df['q50'], bins=50, alpha=0.5, label='Predicted 50th Percentile (Median)')\n",
    "plt.hist(preds_df['q90'], bins=50, alpha=0.5, label='Predicted 90th Percentile')\n",
    "\n",
    "#plot actual returns\n",
    "plt.hist(preds_df['Actual_5d_Return'], bins=50, alpha=0.5, label='Actual 5-Day Returns')\n",
    "\n",
    "plt.title('Distribution of Predicted Quantiles and Actual Returns')\n",
    "plt.xlabel('Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876f880-680d-4ac6-955c-8b9e711da41a",
   "metadata": {
    "id": "2876f880-680d-4ac6-955c-8b9e711da41a"
   },
   "source": [
    "Pinball loss (also called quantile loss) penalizes under-predictions and over-predictions asymmetrically, depending on the quantile.\n",
    "\n",
    "Pinball Loss = max[(q * (y_true - y_pred)), (q-1 * (y_true - y_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b612c9-2bd7-41ad-a5de-70acc03f3768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15b612c9-2bd7-41ad-a5de-70acc03f3768",
    "outputId": "9ac14748-2edc-4a34-aeb3-f457a894ef6c"
   },
   "outputs": [],
   "source": [
    "def pinball_loss(y_true, y_pred, quantile):\n",
    "    error = y_true - y_pred\n",
    "    loss = np.maximum(quantile * error, (quantile - 1)*error)\n",
    "    return np.mean(loss)\n",
    "\n",
    "loss_q10 = pinball_loss(preds_df['Actual_5d_Return'], preds_df['q10'], 0.1)\n",
    "loss_q50 = pinball_loss(preds_df['Actual_5d_Return'], preds_df['q50'], 0.5)\n",
    "loss_q90 = pinball_loss(preds_df['Actual_5d_Return'], preds_df['q90'], 0.9)\n",
    "\n",
    "# Print results\n",
    "print(f\"Pinball Loss for 10th Quantile: {loss_q10:.6f}\")\n",
    "print(f\"Pinball Loss for 50th Quantile: {loss_q50:.6f}\")\n",
    "print(f\"Pinball Loss for 90th Quantile: {loss_q90:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb79e8-3a24-4221-b352-eba76c5c4d8a",
   "metadata": {
    "id": "bfdb79e8-3a24-4221-b352-eba76c5c4d8a"
   },
   "source": [
    "## Trading Simulation With Quantile Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9gkIUlcxMCy",
   "metadata": {
    "id": "h9gkIUlcxMCy"
   },
   "source": [
    "## Trading Simulation Strategy v2\n",
    "   Using run_quantile_backtest function defined above, we run the exact same trading simulation strategy as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35FRg67IwxoI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "35FRg67IwxoI",
    "outputId": "69281651-d2e4-4bed-8de9-519cf14ec486"
   },
   "outputs": [],
   "source": [
    "results = run_quantile_backtest(\n",
    "    prediction_frame=preds_df,\n",
    "    metadata_frame=metadata_test,\n",
    "    initial_cash_amount=100_000,\n",
    "    cash_fraction_at_risk=1,\n",
    "    commission_rate_per_side=0.0001,\n",
    "    bias_buffer_normal_return=0.00057,\n",
    "    skew_buffer_normal_return=0.03,\n",
    "    stop_loss_enabled=True,\n",
    "    holding_period_rows=5,\n",
    "    plot_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad720ae0-3c29-48c7-85d6-a52745abb078",
   "metadata": {},
   "source": [
    "## Results of Predicting Quantiles\n",
    "Overall, what we have seen is excellent evidence that our Transformer model has learned a generalizable quantile-forecasting mapping of 5-day log returns based on financial time series data.\n",
    "\n",
    "1. **Overall Performance**: Our quantile regression baseline had a pinball loss on the order of 1-13 raw scale. Dropping to a pinball loss of 0.008 is a massive improvemenet. Several order of magnitudes (OOMs) improvement. Our goal was to have a 10% reduction in pinball loss compared to the baseline -- we have smashed this goal.\n",
    "2. **Overfitting/Underfitting**: Well it is clear we are not underfitting. In terms of overfitting, our training and validation pinball loss differs by less than 10%, train vs val MAE less than 8%. So the model is generalizing nicely.\n",
    "3. **Residual Error**: A <1% pinball loss means the model's quantile bands are, on average, within about 1 percentage point of the true return. A 3.8% median MAE is quite reasonable given typical 5-day log-return volatility (~2–4%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f90828-7ba5-4023-9b6a-81dc4187efd7",
   "metadata": {},
   "source": [
    "## Results of Simulated Trading Strategy\n",
    "\n",
    "1. **Transformer Model Much Better Returns With Lower Cost**\n",
    "The Transformer strategy turns $100$K into $163.5$K (63.6 % gain) vs. the QR baseline’s $136.9$K (36.9 %). That’s a +26.7 percentage point boost in total return — and at the same time it pays only $2$K in commissions vs. $7.7 K for the baseline.\n",
    "\n",
    "2. **Higher risk-adjusted performance**\n",
    "Despite a slightly higher volatility (16.4 % vs. 15.7 %), the Transformer’s annualised return (14.4 %) is higher. So much so that its Sharpe ratio (≈0.88) exceeds both the baseline’s (≈0.57) and buy-and-hold’s (≈0.79). The Sharpe ratio is a mathematical indication that excess returns over time may be the result of volatility and risk rather than investing skill. A Sharpe ratio between 1-3 is excellent - so our model is nearly there! \n",
    "\n",
    "3. **Comparable draw-downs**\n",
    "Both active strategies actually produce smaller maximum draw-downs (~−18 %) than buy-and-hold (−19.7 %), suggesting the quantile-based methods offer some downside protection.\n",
    "\n",
    "4. **Closing the gap to buy-and-hold**\n",
    "Buy-and-hold still yields the highest absolute return (83 %), but the Transformer strategy closes most of that gap while trading dynamically (turning over capital, paying commissions) and doing so with superior risk-return trade-offs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2afbb8ce3f5e469499584cb69791ce99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_360aca4f65c946a38304ca198f940e6d",
       "IPY_MODEL_3ee904f166334f4dbbc2d82ce25afbd8",
       "IPY_MODEL_5202d7f7af074c79afc1c2f44981272e"
      ],
      "layout": "IPY_MODEL_8b37210b977f4e7ab8104edf450ce933"
     }
    },
    "2d7b2ba7518c4a5ca608dd72a4b56fc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "360aca4f65c946a38304ca198f940e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d7b2ba7518c4a5ca608dd72a4b56fc9",
      "placeholder": "​",
      "style": "IPY_MODEL_dab17d796b7f4f1bb30e2c962b153b2b",
      "value": "100%"
     }
    },
    "3ee904f166334f4dbbc2d82ce25afbd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a975b24264c412d888503e892ca21fe",
      "max": 6145,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_519b51a2dbb34e9ca4419f7e5aadc303",
      "value": 6145
     }
    },
    "4d555dd9ae9543aeb114532dbe23f252": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "519b51a2dbb34e9ca4419f7e5aadc303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5202d7f7af074c79afc1c2f44981272e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be366e498095417288f5a4be015c55bd",
      "placeholder": "​",
      "style": "IPY_MODEL_4d555dd9ae9543aeb114532dbe23f252",
      "value": " 6145/6145 [01:18&lt;00:00, 78.57it/s]"
     }
    },
    "8a975b24264c412d888503e892ca21fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b37210b977f4e7ab8104edf450ce933": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be366e498095417288f5a4be015c55bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dab17d796b7f4f1bb30e2c962b153b2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
